version: '3.8'
services:
  namenode1:
    build:
      context: ./namenode
    image: custom-hadoop-namenode:latest
    container_name: namenode
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_rpc-address=hdfs://namenode:9000
      - HDFS_CONF_dfs_namenode_servicerpc-address=hdfs://namenode:9001
      - HDFS_CONF_dfs_namenode_http-address=hdfs://namenode:50070
      - HDFS_CONF_dfs_namenode_https-address=hdfs://namenode:50470
    ports:
      - "9870:9870" # Expose HDFS UI
    volumes:
      - hadoop_namenode:/hadoop/dfs/name

  datanode1:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode1
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_http-address=datanode1:9864
      - HDFS_CONF_dfs_datanode_address=datanode1:9866
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data

  datanode2:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode2
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:9000
      - HDFS_CONF_dfs_datanode_http-address=datanode2:9864
      - HDFS_CONF_dfs_datanode_address=datanode2:9866
    volumes:
      - hadoop_datanode2:/hadoop/dfs/data

  spark-master:
    image: bde2020/spark-master:2.4.5-hadoop2.7
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_NAME=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
    ports:
      - "8080:8080"
      - "7077:7077"

  spark-worker1:
    image: bde2020/spark-worker:2.4.5-hadoop2.7
    container_name: spark-worker1
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=8881
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "8081:8081"
    depends_on:
      - spark-master

  spark-worker2:
    image: bde2020/spark-worker:2.4.5-hadoop2.7
    container_name: spark-worker2
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_WORKER_PORT=8882
      - SPARK_WORKER_WEBUI_PORT=8082
    ports:
      - "8082:8082"
    depends_on:
      - spark-master
  wordcount:
    build:
      context: ./spark-submit
    image: custom-pyspark-wordcount:latest
    depends_on:
      - spark-master
      - spark-worker1
      - spark-worker2
    entrypoint: ["/spark/bin/spark-submit", "/app/word_count.py", "hdfs://namenode1:9000/user/history/history.txt", "hdfs://namenode1:9000/user/history/output"]
volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
